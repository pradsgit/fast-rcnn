{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import VOCDetection\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "if (torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"gpu found {torch.cuda.get_device_name}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"gpu not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dowload the dataset\n",
    "root = './'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((227, 227))\n",
    "])\n",
    "\n",
    "train_dataset = VOCDetection(root=root, year='2012', image_set='train', download='True', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_MAPPING = {\n",
    "    'aeroplane': 1,\n",
    "    'bicycle': 2,\n",
    "    'bird': 3,\n",
    "    'boat': 4,\n",
    "    'bottle': 5,\n",
    "    'bus': 6,\n",
    "    'car': 7,\n",
    "    'cat': 8,\n",
    "    'chair': 9,\n",
    "    'cow': 10,\n",
    "    'diningtable': 11,\n",
    "    'dog': 12,\n",
    "    'horse': 13,\n",
    "    'motorbike': 14,\n",
    "    'person': 15,\n",
    "    'pottedplant': 16,\n",
    "    'sheep': 17,\n",
    "    'sofa': 18,\n",
    "    'train': 19,\n",
    "    'tvmonitor': 20,\n",
    "}\n",
    "\n",
    "def get_class_id(class_label):\n",
    "    if class_label in CLASS_MAPPING:\n",
    "        return CLASS_MAPPING[class_label]\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes to the vggnet according to the paper\n",
    "\"\"\"\n",
    "1. First, the last max pooling layer is replaced by an RoI\n",
    "pooling layer.\n",
    "2. Second, the networkâ€™s last fully connected layer and softmax \n",
    "(which were trained for 1000-way ImageNet classification) are \n",
    "replaced with the two sibling layers\n",
    "\"\"\"\n",
    "\n",
    "def get_updated_vgg16(use_drop=True):\n",
    "    # load the model\n",
    "    vgg16 = models.vgg16(weights='DEFAULT')\n",
    "    # total number of features are 31 and 30-indexed layer is MaxPool2d\n",
    "    features = list(vgg16.features)[:-1] # remove the last max pooling layer\n",
    "    classifier = list(vgg16.classifier)\n",
    "\n",
    "    # remove last fc layer\n",
    "    del classifier[6]\n",
    "\n",
    "    # ignore Dropout layers\n",
    "    if not use_drop:\n",
    "        # remove dropout layers\n",
    "        del classifier[2]\n",
    "        del classifier[-1]\n",
    "\n",
    "    classifier = nn.Sequential(*classifier)\n",
    "\n",
    "    # TODO: Maybe freeze few conv layers? idk\n",
    "\n",
    "    return nn.Sequential(*features), classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROILayer(nn.Module):\n",
    "    \"\"\"\n",
    "    This layer projects each RoI literally on to the feature map and \n",
    "    for each RoI region on feature map, a fixed size vector is obtained to pass these\n",
    "    fixed size vectors through fc layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size=(7, 7), spatial_scale=0.062):\n",
    "        super(ROILayer, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.spatial_scale = spatial_scale\n",
    "\n",
    "    def forward(self, rois, feature_map):\n",
    "        # rois shape is [N, D]\n",
    "        num_batches, num_channels, img_height, img_width = feature_map.size()\n",
    "        num_rois = rois.size(0) # rois shape would be [N, D]\n",
    "        # init an empty tensor of the desired output shape\n",
    "        output = torch.zeros(num_rois, num_channels, self.output_size[0], self.output_size[1]) # [R, C, H, W]\n",
    "        # for each roi, get corresponding box of feature map across all the channels\n",
    "        for i in range(num_rois):\n",
    "            roi = list(rois[i]) # roi shape would be [x y h w]\n",
    "            # get scaled roi values of bb's top-left and bottom-right coords\n",
    "            roi_start_w, roi_start_h, roi_end_w, roi_end_h = self._scaled_coords([roi[0], roi[1], roi[0]+roi[2], roi[1]+roi[3]], img_height, img_width)\n",
    "            roi_height = roi_end_h - roi_start_h\n",
    "            roi_width = roi_end_w - roi_start_w\n",
    "            # this is the bin size from where max pooling will be performed\n",
    "            bin_height = roi_height / float(self.output_size[0])\n",
    "            bin_width = roi_width / float(self.output_size[1])\n",
    "            batch = 0 if i < 64 else 1\n",
    "            for ph in range(self.output_size[0]):\n",
    "                for pw in range(self.output_size[1]):\n",
    "                    start_h = int(roi_start_h + ph * bin_height)\n",
    "                    start_w = int(roi_start_w + pw * bin_width)\n",
    "                    end_h = int(roi_start_h + (ph + 1) * bin_height)\n",
    "                    end_w = int(roi_start_w + (pw + 1) * bin_width)\n",
    "\n",
    "                    # Clamp the region to be within the bounds of the feature map\n",
    "                    start_h = min(max(start_h, 0), img_height)\n",
    "                    end_h = min(max(end_h, 0), img_height)\n",
    "                    start_w = min(max(start_w, 0), img_width)\n",
    "                    end_w = min(max(end_w, 0), img_width)\n",
    "\n",
    "                    # Perform max pooling over the specified region for each channel\n",
    "                    if start_h < end_h and start_w < end_w:\n",
    "                        region = feature_map[batch, :, start_h:end_h, start_w:end_w]\n",
    "                        output[i, :, ph, pw] = F.adaptive_max_pool2d(region, (1, 1)).view(-1)\n",
    "\n",
    "        return output                      \n",
    "\n",
    "\n",
    "    def _scaled_coords(self, coords, img_height, img_width):\n",
    "        \"\"\"\n",
    "        Since RoI values are calculated in image spatial dimensions scale\n",
    "        and the feature map is in diff spatial dim, scale/transform the values of RoI\n",
    "        to feature map scale from image dim scale\n",
    "        \"\"\"\n",
    "        x1, y1, x2, y2 = coords\n",
    "        x1 = x1 * img_width * self.spatial_scale\n",
    "        y1 = y1 * img_height * self.spatial_scale\n",
    "        x2 = x2 * img_width * self.spatial_scale\n",
    "        y2 = y2 * img_height * self.spatial_scale\n",
    "        return x1, y1, x2, y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBoxRegressionLayer(nn.Module):\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super(BBoxRegressionLayer, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, 4 * num_classes)  # 4 offsets per class\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        # Reshape the output to (batch_size, num_classes, 4)\n",
    "        x = x.view(x.size(0), -1, 4)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastRCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        \"\"\"\n",
    "        A pre-trained network with 3 changes. Considering VGG16 as a pre-trained network\n",
    "        \"\"\"\n",
    "        super(FastRCNN, self).__init__()\n",
    "         # pre-trained CNN's feature extractor and classifer modules\n",
    "        self.feat_extractor, self.classifier = get_updated_vgg16()\n",
    "        # RoI max pooling layer\n",
    "        self.roi_layer = ROILayer((7, 7))\n",
    "        # fully-connected layer \n",
    "        self.fc_layer = nn.Linear(4096, num_classes + 1)\n",
    "        self.bbox = BBoxRegressionLayer(4096, num_classes)\n",
    "\n",
    "    def forward(self, x, rois):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x -> batch of image tensors [B, C, H, W]\n",
    "            rois -> list of regio proposal obejcts each of shape [x1, y1, x2, y2]\n",
    "        \"\"\"\n",
    "        # image shape would be [N, C, H, W]\n",
    "        # pass thru feature extractor\n",
    "        feat_map = self.feat_extractor(x); # this produces a conv feature map\n",
    "        x = self.roi_layer(rois, feat_map) # roi max pooling layer\n",
    "        # flatten the tensors to pass it to next stage that is fully-connected layer\n",
    "        x = torch.flatten(x, start_dim=1) # shape would be [R, 25088]\n",
    "        # pass thru the classifier\n",
    "        x = self.classifier(x)\n",
    "        # two sibling output layers\n",
    "        log_probs = F.softmax(self.fc_layer(x), dim=-1)\n",
    "        regressor = self.bbox(x)\n",
    "        return log_probs, regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zero_index(t):\n",
    "    target_value = 0\n",
    "    temp = (t == target_value).nonzero(as_tuple=False)\n",
    "    if temp.size(0) == 0:\n",
    "        return -1\n",
    "    # print(temp[0])\n",
    "    return int(temp[0])\n",
    "\n",
    "class MultiTaskLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    combination of classification loss and regression bounding box loss.\n",
    "    classification loss is NLL. can use CrossEntropyLoss\n",
    "    regression loss is L1 loss. can use SmoothL1Loss\n",
    "    \"\"\"\n",
    "    def __init__(self, lambda_param=1.0):\n",
    "        super(MultiTaskLoss, self).__init__()\n",
    "        self.lambda_param = lambda_param\n",
    "        self.cls_loss_fn = nn.CrossEntropyLoss()\n",
    "        self.loc_loss_fn = nn.SmoothL1Loss(reduction='sum')\n",
    "    \n",
    "    def forward(self, gt_labels, pred_labels, gt_boxes, pred_boxes):\n",
    "        # calculate classification loss\n",
    "        cls_loss = self.cls_loss_fn(pred_labels, gt_labels)\n",
    "        # if cls_loss.item() is NaN:\n",
    "\n",
    "        print(f\"cls loss {cls_loss.item()}\")\n",
    "        # calculate regresion bounding box loss\n",
    "        # gt_labels is tensor of shape [1, 2, 5, 19, 19, 19, 0, 0, 0, 0]\n",
    "        zero_start_index = get_zero_index(gt_labels)\n",
    "        foreground_obj_labels = gt_labels[:zero_start_index] # shape would 1D tensor of size N\n",
    "        # print(foreground_obj_labels)\n",
    "        # get slices of boxes for only class labels >= 1. considering only foreground obbjects bounding boxes\n",
    "        gt_boxes_slice = gt_boxes[:zero_start_index, :] # shape would be [N, 4]\n",
    "        pred_boxes_slice = pred_boxes[:zero_start_index, :, :] # shape would be [N, C, 4]\n",
    "        # pick a single class label box of each RoI from pred_boxes_slice using foreground_obj_labels values as indices\n",
    "        # sliced_tensor = torch.gather(pred_boxes_slice, 1, foreground_obj_labels.view(-1, 1, 1).expand(-1, -1, pred_boxes_slice.size(2)))\n",
    "        sliced_tensor = pred_boxes_slice[torch.arange(len(foreground_obj_labels)), foreground_obj_labels - 1]\n",
    "        loc_loss = self.loc_loss_fn(gt_boxes_slice, sliced_tensor)\n",
    "        print(f\"loc loss {loc_loss.item()}\")\n",
    "        # add losses\n",
    "        loss = cls_loss + self.lambda_param * loc_loss\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def truncate(f, n):\n",
    "    return math.floor(f * 10 ** n) / 10 ** n\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        box1: a 4-tuple int numbers of top-left and bottom-right corner points of the box [x1, y1, x2, y2]\n",
    "        box2: same as box 1\n",
    "    Returns:\n",
    "        a floating point number of IoU overlap metric\n",
    "    \n",
    "    iou = area of intersection / area of union \n",
    "    \"\"\"\n",
    "    assert len(box1) == len(box2), \"Length of box1 and box2 tuples should be equal\"\n",
    "    # find the top-left and bottom right points of the intersection rectangle\n",
    "    a1 = max(box1[0], box2[0])\n",
    "    a2 = max(box1[1], box2[1])\n",
    "    a3 = min(box1[2], box2[2])\n",
    "    a4 = min(box1[3], box2[3])\n",
    "\n",
    "    # calculate height and width of the interseced rect\n",
    "    width = max(0, a3 - a1)\n",
    "    height = max(0, a4 - a2)\n",
    "\n",
    "    area_int = width * height\n",
    "\n",
    "    if area_int == 0:\n",
    "        return float(0)\n",
    "    \n",
    "    # box1 dimensions\n",
    "    box1_w = box1[2] - box1[0]\n",
    "    box1_h = box1[3] - box1[1]\n",
    "\n",
    "    # box2 dimensions\n",
    "    box2_w = box2[2] - box2[0]\n",
    "    box2_h = box2[3] - box2[1]\n",
    "\n",
    "    area_uni = (box1_w * box1_h) + (box2_w * box2_h) - area_int\n",
    "\n",
    "    return truncate(area_int / area_uni, 2)\n",
    "\n",
    "\n",
    "# calculate_iou([ 58., 0., 227., 144.], [45, 37, 202, 227])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou_lists(proposal_boxes, gt_boxes):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        proposal_boxes: list of bounding boxes of object proposals(2D array). each element if of shape (x1, y1, x2, y2) \n",
    "        gt_boxes: list of tuples (ground truth/actual bounding boxes of objects, class_label), each ground truth box is of shape (x1, y1, x2, y2)\n",
    "    Returns:\n",
    "        two lists. \n",
    "        list1 contains rois where IoU overlap is >= 0.5\n",
    "        list2 where IoU overlap is in interval [0.1, 0.5) ie (>= 0.1 and < 0.5)\n",
    "    \"\"\"\n",
    "    positive_rois = [] # for list1\n",
    "    negative_rois = [] # for list2\n",
    "\n",
    "    for gt_box in gt_boxes:\n",
    "        for pp_box in proposal_boxes:\n",
    "            overlap_val = calculate_iou(pp_box, gt_box[0])\n",
    "            if overlap_val >= 0.5:\n",
    "                # print(f'pp_box is {pp_box} and gt_box is {gt_box}')\n",
    "                # append (box, gt_label, gt_box) to positive_rois list\n",
    "                positive_rois.append((pp_box, get_class_id(gt_box[1]), gt_box[0]))\n",
    "            elif overlap_val >= 0.1 and overlap_val < 0.5:\n",
    "                negative_rois.append((pp_box, 0))\n",
    "    \n",
    "    return positive_rois, negative_rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proposals(image):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        image => 3D numpy array of shape [H W C]\n",
    "    returns:\n",
    "        RoI proposals using selective search\n",
    "    \"\"\"\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "\n",
    "    gs = cv2.ximgproc.segmentation.createGraphSegmentation(0.8, 150, 100)\n",
    "    ss.addGraphSegmentation(gs)\n",
    "\n",
    "    strategy_color = cv2.ximgproc.segmentation.createSelectiveSearchSegmentationStrategyColor() # color strategy \n",
    "    strategy_fill = cv2.ximgproc.segmentation.createSelectiveSearchSegmentationStrategyFill() # fill strategy\n",
    "    strategy_size = cv2.ximgproc.segmentation.createSelectiveSearchSegmentationStrategySize() # size strat\n",
    "    strategy_texture = cv2.ximgproc.segmentation.createSelectiveSearchSegmentationStrategyTexture() # texture strat\n",
    "    strategy_multiple = cv2.ximgproc.segmentation.createSelectiveSearchSegmentationStrategyMultiple(\n",
    "        strategy_color, strategy_fill, strategy_size, strategy_texture) # linear combination of all the above strats\n",
    "    ss.addStrategy(strategy_multiple) # add this strat module to selective search\n",
    "    \n",
    "    ss.addImage(image)\n",
    "    ss.setBaseImage(image)\n",
    "    ss.switchToSelectiveSearchQuality()\n",
    "    # get the bounding boxes with each of shape (x, y, w, h)\n",
    "    proposals = ss.process()\n",
    "    # convert the bounding boxes with each of shape (x, y, w, h) to (x1, y1, x2, y2)\n",
    "    proposals = [[box[0], box[1], box[0]+box[2], box[1]+box[3]] for box in proposals]\n",
    "    return proposals    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct RoIs from an image\n",
    "def selective_search(image, bnb_boxes, limit=64, iou=True):\n",
    "    \"\"\"\n",
    "    Takes in image tensor and its ground truth boxes and returns Region Proposals tensor\n",
    "    \"\"\"\n",
    "    assert type(image) == torch.Tensor, \"Expected image input to be of type tensor\"\n",
    "    # convert tensor to numpy and reshape the array to [H W C]\n",
    "    # print(f\"Image shape in selective seach is ${image.shape}\")\n",
    "    image = image.permute(1, 2, 0).detach().numpy()\n",
    "    # print(f\"Image shape in selective seach is ${image.shape}\")\n",
    "    proposals = get_proposals(image)\n",
    "\n",
    "    if not iou:\n",
    "        return torch.tensor(proposals, dtype=torch.float32)\n",
    "    final_boxes, gt_labels, gt_boxes = [], [], []\n",
    "\n",
    "    # get IoU specific object proposal groups. group 1 IoU value >= 0.5 and group 2 is in range [0.1, 0.5)\n",
    "    positive_rois, negative_rois = get_iou_lists(proposals, bnb_boxes)\n",
    "    positive_roi_boxes, positive_roi_labels, postive_roi_gt_boxes = [], [], []\n",
    "    negative_roi_boxes, negative_roi_labels = [], []\n",
    "\n",
    "    if len(positive_rois) > 0:\n",
    "        positive_roi_boxes, positive_roi_labels, positive_roi_gt_boxes = zip(*positive_rois) # each unpacked value is a tuple, to be converted to list\n",
    "        # get 25% of positive_rois\n",
    "        positive_rois_limit = min(int(0.25 * limit), len(positive_rois))\n",
    "        final_boxes.extend(list(positive_roi_boxes)[:positive_rois_limit])\n",
    "        gt_labels.extend(list(positive_roi_labels)[:positive_rois_limit])\n",
    "        gt_boxes.extend(list(positive_roi_gt_boxes)[:positive_rois_limit])\n",
    "\n",
    "\n",
    "    remaining_limit = limit - len(final_boxes)\n",
    "    \n",
    "    if len(negative_rois) > 0:\n",
    "        negative_roi_boxes, negative_roi_labels = zip(*negative_rois) # each unpacked value is a tuple, to be converted to list\n",
    "\n",
    "        negative_rois_limit = min(remaining_limit, len(negative_rois))\n",
    "        final_boxes.extend(list(negative_roi_boxes)[:negative_rois_limit])\n",
    "        gt_labels.extend(list(negative_roi_labels)[:negative_rois_limit])\n",
    "        for _ in range(negative_rois_limit):\n",
    "            gt_boxes.append([0, 0, 0, 0])\n",
    "\n",
    "    # hackiest code below. be cautious!\n",
    "    # if the final boxes length does not reach limit\n",
    "    while (len(final_boxes) < limit):\n",
    "        temp_limit = limit - len(final_boxes)\n",
    "        final_boxes.extend(proposals[:temp_limit])\n",
    "        gt_labels.extend([0] * temp_limit)\n",
    "        for _ in range(temp_limit):\n",
    "            gt_boxes.append([0, 0, 0, 0])\n",
    "\n",
    "    if len(final_boxes) == 0:\n",
    "        # what to do in this case?\n",
    "        final_boxes = proposals[:limit] # what if proposals are less than limit\n",
    "        gt_labels = [0] * limit\n",
    "        for _ in range(limit):\n",
    "            gt_boxes.append([0, 0, 0, 0])\n",
    "\n",
    "    assert len(final_boxes) == limit, f\"Error length of final boxes is {len(final_boxes)}\"\n",
    "    if len(final_boxes) > limit:\n",
    "        # clip them\n",
    "        final_boxes = final_boxes[:limit]\n",
    "    if len(gt_labels) > limit:\n",
    "        # clip them\n",
    "        gt_labels = gt_labels[:limit]\n",
    "    if len(gt_boxes) > limit:\n",
    "        # clip them\n",
    "        gt_boxes = gt_boxes[:limit]\n",
    "    assert len(gt_boxes) == limit, f\"gt_boxes limit is greater than limit {limit}\"\n",
    "    return torch.tensor(final_boxes, dtype=torch.float32), torch.tensor(gt_labels, dtype=torch.float32), torch.tensor(gt_boxes, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLaoder\n",
    "# all torchvision datasets are subclass of torch.utils.data.Dataset. so no need of creating a new Dataset class\n",
    "\n",
    "def get_bnd_boxes(annotation_data, height, width, rescale=False):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        annotation_data => annaotation data object from VOC dataset\n",
    "        returns 2d array of [[x1, y1, x2, y2], ...]\n",
    "    \"\"\"\n",
    "    actual_image_dim = annotation_data[\"annotation\"][\"size\"]\n",
    "    h_ratio = 1\n",
    "    w_ratio = 1\n",
    "\n",
    "    if rescale:\n",
    "        h_ratio = truncate(height / int(actual_image_dim[\"height\"]), 2)\n",
    "        w_ratio = truncate(width / int(actual_image_dim[\"width\"]), 2)\n",
    "\n",
    "\n",
    "    bndboxes = [obj for obj in annotation_data[\"annotation\"][\"object\"]]\n",
    "\n",
    "    # print(bndboxes)\n",
    "    # clip values to height and width of image tensor on which is RoIs are produced\n",
    "    return [\n",
    "        ([\n",
    "            min(int(float(box[\"bndbox\"][\"xmin\"]) * w_ratio), width), \n",
    "            min(int(float(box[\"bndbox\"][\"ymin\"]) * h_ratio), height), \n",
    "            min(int(float(box[\"bndbox\"][\"xmax\"]) * w_ratio), width), \n",
    "            min(int(float(box[\"bndbox\"][\"ymax\"]) * h_ratio), height)\n",
    "        ], box[\"name\"])\n",
    "        for box in bndboxes\n",
    "    ]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    images, rois, gt_labels, gt_boxes = [], [], [], []\n",
    "    for (img_tensor, annotation_data) in batch:\n",
    "        images.append(img_tensor)\n",
    "        bnd_boxes = get_bnd_boxes(annotation_data, img_tensor.shape[1], img_tensor.shape[2])\n",
    "        # print(bnd_boxes)\n",
    "        # calculate RoIs of each image and return the RoIs along with stacked tensors\n",
    "        final_rois, final_labels, final_gt_boxes = selective_search(img_tensor, bnd_boxes)\n",
    "        rois.append(final_rois)\n",
    "        gt_labels.append(final_labels)\n",
    "        gt_boxes.append(final_gt_boxes) # bnd_boxes is list of tuples\n",
    "    return torch.stack(images, dim=0), torch.vstack(rois), torch.hstack(gt_labels), torch.vstack(gt_boxes)  # always stack the tensors\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastRCNN(num_classes=len(CLASS_MAPPING.keys()))\n",
    "criterion = MultiTaskLoss(lambda_param=1.0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    epoch_loss = 0.0\n",
    "    losses = []\n",
    "    for (image_tensors, rois, gt_labels, gt_boxes) in train_dl:\n",
    "        pred_labels, pred_boxes = model(image_tensors, rois)\n",
    "        loss = criterion(gt_labels.long(), pred_labels, gt_boxes, pred_boxes)\n",
    "        losses.append(loss.item())\n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"Batch loss is {loss.item()}\")\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_dl)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
